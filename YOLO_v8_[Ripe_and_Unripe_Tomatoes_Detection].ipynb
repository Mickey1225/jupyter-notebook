{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8796473,
          "sourceType": "datasetVersion",
          "datasetId": 5289371
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "YOLO v8 [Ripe and Unripe Tomatoes Detection]",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mickey1225/jupyter-notebook/blob/main/YOLO_v8_%5BRipe_and_Unripe_Tomatoes_Detection%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "sumn2u_riped_and_unriped_tomato_dataset_path = kagglehub.dataset_download('sumn2u/riped-and-unriped-tomato-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Tsfqh6UQ7Yyh"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "The given dataset is provided in the following format:\n",
        "\n",
        "```sh\n",
        "Riped and Unriped Tomato Dataset/\n",
        "├── Images/\n",
        "│   ├── riped_tomato_1.jpeg\n",
        "│   ├── riped_tomato_10.jpeg\n",
        "│   ├── riped_tomato_11.jpeg\n",
        "│   ├── riped_tomato_12.jpeg\n",
        "│   └── ...\n",
        "└── Labels/\n",
        "    ├── riped_tomato_1.txt\n",
        "    ├── riped_tomato_10.txt\n",
        "    ├── riped_tomato_11.txt\n",
        "    ├── riped_tomato_12.txt\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "The recommended format differs from what we have, so we will split the dataset to support the [YOLO fromat](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format). For this, we will import some libraries.\n"
      ],
      "metadata": {
        "id": "qON3RKBO7Yyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split # To split the data into train and test\n",
        "import cv2\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob"
      ],
      "metadata": {
        "trusted": true,
        "id": "iie8AtBm7Yyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base path for the dataset\n",
        "base_data_dir = os.path.join(\"/kaggle\",\"input\", \"riped-and-unriped-tomato-dataset\")\n",
        "print(f\"Base Data Directory: {base_data_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "mo5o2IdI7Yyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define specific paths for images and labels\n",
        "images_dir = os.path.join(base_data_dir, \"Riped and Unriped Tomato Dataset\", \"Images\")\n",
        "labels_dir = os.path.join(base_data_dir, \"Riped and Unriped Tomato Dataset\", \"labels\")\n",
        "\n",
        "print(f\"Images Dir: {images_dir}, Labels Dir: {labels_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "C-q0T3L_7Yyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the file extensions that we have in image directory"
      ],
      "metadata": {
        "id": "5Hn3kpGj7Yyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all files in the directory\n",
        "files = glob.glob(os.path.join(images_dir, '*'))\n",
        "\n",
        "# Collect unique file extensions\n",
        "extensions = set()\n",
        "for file in files:\n",
        "    _, ext = os.path.splitext(file)\n",
        "    extensions.add(ext.lower())  # Convert to lowercase to handle case sensitivity\n",
        "\n",
        "# Print all unique extensions found\n",
        "print(\"Unique File Extensions:\")\n",
        "for ext in extensions:\n",
        "    print(ext)"
      ],
      "metadata": {
        "trusted": true,
        "id": "sXYJxw6E7Yyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have all JPEG files. The below function loads images and corresponding annotation paths from directories."
      ],
      "metadata": {
        "id": "n9GsYW0u7Yyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(images_dir, labels_dir):\n",
        "    images = []\n",
        "    annotations = []\n",
        "\n",
        "    for filename in os.listdir(images_dir):\n",
        "        if filename.endswith('.jpeg'):\n",
        "            image_path = os.path.join(images_dir, filename)\n",
        "            annotation_path = os.path.join(labels_dir, filename.replace('.jpeg', '.txt'))\n",
        "\n",
        "            if os.path.exists(annotation_path):\n",
        "                images.append(image_path)\n",
        "                annotations.append(annotation_path)\n",
        "\n",
        "    return images, annotations"
      ],
      "metadata": {
        "trusted": true,
        "id": "iyUq9SZn7Yy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, annotations = load_data(images_dir, labels_dir)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4FgKrNWE7Yy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualizes a random sample of images. Let's create a `visualize_samples` function."
      ],
      "metadata": {
        "id": "BTHM_sD97Yy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(images, num_samples=5):\n",
        "    fig, axs = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        image = Image.open(images[i])\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].axis('off')\n",
        "        axs[i].set_title(f'Sample {i+1}')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2RK-QGRS7Yy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_samples(images)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xNpuutwT7Yy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate and visualize the average height and width of the images in the dataset."
      ],
      "metadata": {
        "id": "MlFstiv07Yy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_size(images_dir):\n",
        "    total_width = 0\n",
        "    total_height = 0\n",
        "    total_images = 0\n",
        "\n",
        "    for filename in os.listdir(images_dir):\n",
        "        if filename.endswith('.jpeg'):  # assuming images are in .jpeg format\n",
        "            image_path = os.path.join(images_dir, filename)\n",
        "            image = Image.open(image_path)\n",
        "            width, height = image.size\n",
        "            total_width += width\n",
        "            total_height += height\n",
        "            total_images += 1\n",
        "\n",
        "    if total_images > 0:\n",
        "        average_width = total_width / total_images\n",
        "        average_height = total_height / total_images\n",
        "        return average_width, average_height\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "trusted": true,
        "id": "MLeR0y487Yy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_average_size(average_width, average_height):\n",
        "    categories = ['Average Width', 'Average Height']\n",
        "    values = [average_width, average_height]\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(categories, values, color=['skyblue', 'lightgreen'])\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Pixels')\n",
        "    plt.title('Average Image Dimensions')\n",
        "    plt.ylim(0, max(values) * 1.1)\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Nx84Gz_p7Yy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_width, average_height = calculate_average_size(images_dir)\n",
        "\n",
        "if average_width and average_height:\n",
        "    print(f\"Average Image Width: {average_width:.2f} pixels\")\n",
        "    print(f\"Average Image Height: {average_height:.2f} pixels\")\n",
        "\n",
        "    # Plot average size\n",
        "    plot_average_size(average_width, average_height)\n",
        "else:\n",
        "    print(\"No images found or unable to calculate average size.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ac5Xy9fA7Yy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are also going to plot the distribution of classes ('ripe' and 'unripe') so, let's create a seperate function for this."
      ],
      "metadata": {
        "id": "r_Lkurc-7Yy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(annotations):\n",
        "    classes = {'ripe': 0, 'unripe': 0}\n",
        "\n",
        "    for annotation_file in annotations:\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                class_label = line.split()[0]  # assuming YOLO format with class label as first entry\n",
        "                if class_label == '0':\n",
        "                    classes['unripe'] += 1\n",
        "                elif class_label == '1':\n",
        "                    classes['ripe'] += 1\n",
        "    # Print the class counts\n",
        "    for cls, count in classes.items():\n",
        "        print(f\"{cls}: {count}\")\n",
        "\n",
        "    # Plotting\n",
        "    plt.bar(classes.keys(), classes.values())\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "MxmP1uoQ7Yy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_class_distribution(annotations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mspyytKn7Yy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the RGB distributions of our datasets. Generally, the ripe tomatoes will have higher intensity in the red channel, whereas unripe tomatoes might have higher intensity in the green channel."
      ],
      "metadata": {
        "id": "L8T4QcC-7Yy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob"
      ],
      "metadata": {
        "trusted": true,
        "id": "MKWo7wjl7Yy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_annotations(annotation_path):\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        return file.read().strip()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RcQXEYbG7Yy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is not None:\n",
        "        return img\n",
        "    return np.array([])"
      ],
      "metadata": {
        "trusted": true,
        "id": "0s4Lmi-d7Yy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_in_batches(images, max_images=50, batch_size=10):\n",
        "    \"\"\"Process images in batches to reduce memory usage.\"\"\"\n",
        "    r_values, g_values, b_values = [], [], []\n",
        "\n",
        "    # Limit the number of images to max_images\n",
        "    images = images[:max_images]\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i + batch_size]\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            img_list = list(executor.map(read_image, batch))\n",
        "\n",
        "        for img in img_list:\n",
        "            if img.size > 0:\n",
        "                r_values.extend(img[:, :, 0].flatten())\n",
        "                g_values.extend(img[:, :, 1].flatten())\n",
        "                b_values.extend(img[:, :, 2].flatten())\n",
        "\n",
        "    return r_values, g_values, b_values\n",
        "\n",
        "def plot_rgb_distribution(images, category_name, max_images=50, batch_size=10):\n",
        "    \"\"\"Plot RGB distribution for the given images.\"\"\"\n",
        "    r_values, g_values, b_values = process_images_in_batches(images, max_images, batch_size)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Red Channel\n",
        "    plt.subplot(1, 3, 1)\n",
        "    r_hist, r_bins = np.histogram(r_values, bins=256, range=(0, 256))\n",
        "    plt.bar(r_bins[:-1], r_hist, width=1, color='red', alpha=0.5)\n",
        "    max_r_freq = r_hist.max()\n",
        "    plt.axhline(max_r_freq, color='red', linestyle='--', label=f'Max Frequency: {max_r_freq}')\n",
        "    plt.title(f'{category_name} - Red Channel')\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    # Green Channel\n",
        "    plt.subplot(1, 3, 2)\n",
        "    g_hist, g_bins = np.histogram(g_values, bins=256, range=(0, 256))\n",
        "    plt.bar(g_bins[:-1], g_hist, width=1, color='green', alpha=0.5)\n",
        "    max_g_freq = g_hist.max()\n",
        "    plt.axhline(max_g_freq, color='green', linestyle='--', label=f'Max Frequency: {max_g_freq}')\n",
        "    plt.title(f'{category_name} - Green Channel')\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    # Blue Channel\n",
        "    plt.subplot(1, 3, 3)\n",
        "    b_hist, b_bins = np.histogram(b_values, bins=256, range=(0, 256))\n",
        "    plt.bar(b_bins[:-1], b_hist, width=1, color='blue', alpha=0.5)\n",
        "    max_b_freq = b_hist.max()\n",
        "    plt.axhline(max_b_freq, color='blue', linestyle='--', label=f'Max Frequency: {max_b_freq}')\n",
        "    plt.title(f'{category_name} - Blue Channel')\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6NNG_DYD7Yy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate ripe and unripe tomatoes based on annotations\n",
        "ripe_images = []\n",
        "unripe_images = []\n",
        "\n",
        "for img_path, annotation_path in zip(images, annotations):\n",
        "    annotation = read_annotations(annotation_path)\n",
        "    if annotation.startswith('1'):\n",
        "        ripe_images.append(img_path)\n",
        "    elif annotation.startswith('0'):\n",
        "        unripe_images.append(img_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y0aJGesU7Yy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Print the number of images in each category\n",
        "print(f\"Number of ripe images: {len(ripe_images)}\")\n",
        "print(f\"Number of unripe images: {len(unripe_images)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NLQjekSf7Yy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot RGB distribution for ripe tomatoes\n",
        "plot_rgb_distribution(ripe_images, 'Ripe Tomatoes')"
      ],
      "metadata": {
        "trusted": true,
        "id": "uy97hYQV7YzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot RGB distribution for unripe tomatoes\n",
        "plot_rgb_distribution(unripe_images, 'Unripe Tomatoes')"
      ],
      "metadata": {
        "trusted": true,
        "id": "sUslXgTS7YzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above, we can see that the **RED** color distribution is higher for ripe tomatoes whereas **Green** for unripe ones, indicating that ripe tomatoes have stronger presenence of red color in their images and green color for unripe ones. Lets apply various edge detection technique and see which better captures the tomato features.\n",
        "\n",
        "* **Adaptive Thresholding**: Enhances contrast to highlight edges.\n",
        "* **Laplacian Detection**: Detects rapid intensity changes.\n",
        "* **Canny Detection**: Identifies detailed edges through gradient analysis.\n",
        "* **Sobel Detection**: Finds edges using gradient in x and y directions."
      ],
      "metadata": {
        "id": "bcDmq8fK7YzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_edge_density(edges):\n",
        "    return np.sum(edges) / edges.size"
      ],
      "metadata": {
        "trusted": true,
        "id": "cefolduo7YzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_random_images_with_edges(basefolder, num_images=5):\n",
        "    # Define image folder path and get list of image files\n",
        "    image_folder = basefolder\n",
        "    image_files = glob(os.path.join(image_folder, \"*.jpeg\"))\n",
        "\n",
        "    # Check if there are enough images to display\n",
        "    if len(image_files) < num_images:\n",
        "        print(f\"Not enough images in {image_folder} to display {num_images}.\")\n",
        "        return\n",
        "\n",
        "    # Randomly select images\n",
        "    selected_images = random.sample(image_files, num_images)\n",
        "\n",
        "    # Create subplots to display the images\n",
        "    fig, axes = plt.subplots(num_images, 5, figsize=(20, 4 * num_images))\n",
        "\n",
        "    for idx, image_file in enumerate(selected_images):\n",
        "        # Read and preprocess the image\n",
        "        img_path = os.path.join(image_folder, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply various edge detection techniques\n",
        "        # Adaptive Thresholding\n",
        "        adaptive_thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 3)\n",
        "\n",
        "        # Laplacian Edge Detection\n",
        "        lap = cv2.Laplacian(gray_img, cv2.CV_64F)\n",
        "        lap = np.uint8(np.absolute(lap))\n",
        "        lap_rgb = cv2.cvtColor(lap, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Canny Edge Detection\n",
        "        mean_intensity = np.mean(gray_img)\n",
        "        std_intensity = np.std(gray_img)\n",
        "        lower_threshold = int(max(0, mean_intensity - 2 * std_intensity))\n",
        "        upper_threshold = int(min(255, mean_intensity + 2 * std_intensity))\n",
        "        edges = cv2.Canny(gray_img, lower_threshold, upper_threshold)\n",
        "\n",
        "        # Sobel Edge Detection\n",
        "        sobelx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0)\n",
        "        sobely = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1)\n",
        "        combined_sobel = cv2.bitwise_or(cv2.convertScaleAbs(sobelx), cv2.convertScaleAbs(sobely))\n",
        "        combined_sobel_rgb = cv2.cvtColor(combined_sobel, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Display Original Image\n",
        "        axes[idx, 0].imshow(img_rgb)\n",
        "        axes[idx, 0].set_title('Original')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Display Adaptive Thresholded Image\n",
        "        axes[idx, 1].imshow(adaptive_thresh, cmap='gray')\n",
        "        axes[idx, 1].set_title('Adaptive Threshold')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Display Laplacian Edge Detection Image\n",
        "        axes[idx, 2].imshow(lap_rgb)\n",
        "        axes[idx, 2].set_title('Laplacian')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Display Canny Edge Detection Image\n",
        "        axes[idx, 3].imshow(edges, cmap='gray')\n",
        "        axes[idx, 3].set_title('Canny')\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "        # Display Sobel Edge Detection Image\n",
        "        axes[idx, 4].imshow(combined_sobel_rgb)\n",
        "        axes[idx, 4].set_title('Sobel')\n",
        "        axes[idx, 4].axis('off')\n",
        "\n",
        "    # Adjust layout and show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BiYzGH6Z7YzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_random_images_with_edges(images_dir, num_images=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "82YzjAHi7YzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the edge density of above edge detection. For this, we will plot the frequency vs edge density of the above applied techniques."
      ],
      "metadata": {
        "id": "yG3O_JaJ7YzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_edge_density(basefolder, num_images=5):\n",
        "    image_files = glob(os.path.join(basefolder, \"*.jpeg\"))\n",
        "    if len(image_files) < num_images:\n",
        "        print(f\"Not enough images in {basefolder} to display {num_images}.\")\n",
        "        return\n",
        "\n",
        "    selected_images = random.sample(image_files, num_images)\n",
        "    densities = {'Adaptive Threshold': [], 'Laplacian': [], 'Canny': [], 'Sobel': []}\n",
        "\n",
        "    for img_file in selected_images:\n",
        "        img_path = os.path.join(basefolder, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Adaptive Thresholding\n",
        "        adaptive_thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 3)\n",
        "        densities['Adaptive Threshold'].append(calculate_edge_density(adaptive_thresh))\n",
        "\n",
        "        # Laplacian Edge Detection\n",
        "        lap = cv2.Laplacian(gray_img, cv2.CV_64F)\n",
        "        lap = np.uint8(np.absolute(lap))\n",
        "        densities['Laplacian'].append(calculate_edge_density(lap))\n",
        "\n",
        "        # Canny Edge Detection\n",
        "        mean_intensity = np.mean(gray_img)\n",
        "        std_intensity = np.std(gray_img)\n",
        "        lower_threshold = int(max(0, mean_intensity - 2 * std_intensity))\n",
        "        upper_threshold = int(min(255, mean_intensity + 2 * std_intensity))\n",
        "        edges = cv2.Canny(gray_img, lower_threshold, upper_threshold)\n",
        "        densities['Canny'].append(calculate_edge_density(edges))\n",
        "\n",
        "        # Sobel Edge Detection\n",
        "        sobelx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0)\n",
        "        sobely = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1)\n",
        "        combined_sobel = cv2.bitwise_or(cv2.convertScaleAbs(sobelx), cv2.convertScaleAbs(sobely))\n",
        "        densities['Sobel'].append(calculate_edge_density(combined_sobel))\n",
        "\n",
        "    # Plot the average edge densities for each method\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for method, density_values in densities.items():\n",
        "        plt.hist(density_values, bins=10, alpha=0.5, label=method)\n",
        "\n",
        "    plt.title('Edge Density Comparison')\n",
        "    plt.xlabel('Edge Density')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "x1LV4U877YzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_edge_density(images_dir, num_images=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CJeZrGC27YzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we can see the frequency and edge density of each technique. For detecting finer details or stronger edges, higher frequencies are better.\n",
        "\n",
        "To count the number of images containing both 'ripe' and 'unripe' tomatoes let's create a count_mixed_images function that takes list of annotations as an arguments."
      ],
      "metadata": {
        "id": "0rWVQE5Z7YzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_mixed_images(annotations):\n",
        "    mixed_images_count = 0\n",
        "\n",
        "    for annotation_file in annotations:\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            has_ripe = False\n",
        "            has_unripe = False\n",
        "            for line in lines:\n",
        "                class_label = int(line.split()[0])\n",
        "                if class_label == 0:\n",
        "                    has_unripe = True\n",
        "                elif class_label == 1:\n",
        "                    has_ripe = True\n",
        "            if has_ripe and has_unripe:\n",
        "                mixed_images_count += 1\n",
        "\n",
        "    return mixed_images_count"
      ],
      "metadata": {
        "trusted": true,
        "id": "m8SOdYhK7YzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_images_count = count_mixed_images(annotations)\n",
        "\n",
        "print(f\"Mixed Images Count: {mixed_images_count}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "lsWWsKV57YzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the count of images containing both 'ripe' and 'unripe' tomatoes. Let's have a seperate function."
      ],
      "metadata": {
        "id": "Oj0HPfs17YzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mixed_images_count(mixed_images_count):\n",
        "    categories = ['Mixed Images']\n",
        "    counts = [mixed_images_count]\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(categories, counts, color='skyblue')\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title('Images Containing Both Ripe and Unripe Tomatoes')\n",
        "    plt.ylim(0, max(counts) * 1.1)\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CnIaSDSh7YzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting mixed images count\n",
        "plot_mixed_images_count(mixed_images_count)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9c1XOfCC7YzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains images that has both ripe and unripe tomatoes so, let's create a function to find and display sample images that has both."
      ],
      "metadata": {
        "id": "IS4jEdoW7YzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_mixed_images(images_dir, labels_dir, num_samples=3):\n",
        "    # Load dataset\n",
        "    images, annotations = load_data(images_dir, labels_dir)\n",
        "\n",
        "    # Initialize a list to store paths of mixed images\n",
        "    mixed_image_paths = []\n",
        "\n",
        "    # Iterate through annotations to find images with both classes\n",
        "    for i, annotation_file in enumerate(annotations):\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            has_ripe = False\n",
        "            has_unripe = False\n",
        "            for line in lines:\n",
        "                class_label = int(line.split()[0])  # assuming YOLO format with class label as first entry\n",
        "                if class_label == 0:\n",
        "                    has_unripe = True\n",
        "                elif class_label == 1:\n",
        "                    has_ripe = True\n",
        "            if has_ripe and has_unripe:\n",
        "                mixed_image_paths.append(images[i])\n",
        "\n",
        "        # Stop when enough samples are found\n",
        "        if len(mixed_image_paths) >= num_samples:\n",
        "            break\n",
        "\n",
        "    # Display sample images\n",
        "    if mixed_image_paths:\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        for i, image_path in enumerate(random.sample(mixed_image_paths, num_samples)):\n",
        "            image = Image.open(image_path)\n",
        "            plt.subplot(1, num_samples, i + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Sample {i+1}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No images found with both ripe and unripe tomatoes.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "PITTJqMI7YzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample_mixed_images(images_dir, labels_dir, num_samples=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "LtyP6K4a7YzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To draw the bounding boxes on an image based on annotations, we will be using draw_bounding_boxes function."
      ],
      "metadata": {
        "id": "beEv4Qrm7YzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_boxes(image_path, annotation_path):\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        for line in f:\n",
        "            class_label, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "            # Convert YOLO format to bounding box coordinates\n",
        "            img_width, img_height = image.size\n",
        "            x_center *= img_width\n",
        "            y_center *= img_height\n",
        "            width *= img_width\n",
        "            height *= img_height\n",
        "            x_min = x_center - (width / 2)\n",
        "            x_max = x_center + (width / 2)\n",
        "            y_min = y_center - (height / 2)\n",
        "            y_max = y_center + (height / 2)\n",
        "            # Draw rectangle\n",
        "            if class_label == 0:\n",
        "                color = 'green'  # unripe\n",
        "            else:\n",
        "                color = 'red'  # ripe\n",
        "            draw.rectangle([x_min, y_min, x_max, y_max], outline=color, width=2)\n",
        "    return image"
      ],
      "metadata": {
        "trusted": true,
        "id": "2J0q-8tw7YzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since it's possible to have both ripe and unripe tomatoes in a single image, let's create a function to display these images with bounding boxes."
      ],
      "metadata": {
        "id": "Ce4wgp1M7YzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_mixed_images_with_bounding_boxes(images_dir, labels_dir, num_samples=3):\n",
        "    # Load dataset\n",
        "    images, annotations = load_data(images_dir, labels_dir)\n",
        "\n",
        "    # Initialize a list to store paths of mixed images\n",
        "    mixed_image_paths = []\n",
        "\n",
        "    # Iterate through annotations to find images with both classes\n",
        "    for i, annotation_file in enumerate(annotations):\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            has_ripe = False\n",
        "            has_unripe = False\n",
        "            for line in lines:\n",
        "                class_label = int(line.split()[0])  # assuming YOLO format with class label as first entry\n",
        "                if class_label == 0:\n",
        "                    has_unripe = True\n",
        "                elif class_label == 1:\n",
        "                    has_ripe = True\n",
        "            if has_ripe and has_unripe:\n",
        "                mixed_image_paths.append((images[i], annotations[i]))\n",
        "\n",
        "        # Stop when enough samples are found\n",
        "        if len(mixed_image_paths) >= num_samples:\n",
        "            break\n",
        "\n",
        "    # Display sample images\n",
        "    if mixed_image_paths:\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        for i, (image_path, annotation_path) in enumerate(random.sample(mixed_image_paths, num_samples)):\n",
        "            original_image = Image.open(image_path)\n",
        "            annotated_image = draw_bounding_boxes(image_path, annotation_path)\n",
        "\n",
        "            plt.subplot(2, num_samples, i + 1)\n",
        "            plt.imshow(original_image)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Original {i+1}')\n",
        "\n",
        "            plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "            plt.imshow(annotated_image)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Annotated {i+1}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No images found with both ripe and unripe tomatoes.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "FP7rK_Sv7YzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample_mixed_images_with_bounding_boxes(images_dir, labels_dir, num_samples=3)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pWnjgkA67YzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle potential train/test subdirectories\n",
        "working_dir = os.path.join(\"/kaggle\",\"working\")\n",
        "train_images_dir = os.path.join(working_dir, \"train\", \"images\")\n",
        "train_labels_dir = os.path.join(working_dir, \"train\", 'labels')\n",
        "test_images_dir = os.path.join(working_dir,\"test\", 'images')\n",
        "test_labels_dir = os.path.join(working_dir, \"test\", 'labels')\n",
        "\n",
        "print(f\"Train Images Dir: {train_images_dir}, Train Labels Dir: {train_labels_dir}\")\n",
        "print(f\"Test Images Dir: {test_images_dir}, Test Labels Dir: {test_labels_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LQ5wKq6R7YzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test directories if they don't exist (using os.makedirs with exist_ok=True)\n",
        "os.makedirs(train_images_dir, exist_ok=True)\n",
        "os.makedirs(train_labels_dir, exist_ok=True)\n",
        "os.makedirs(test_images_dir, exist_ok=True)\n",
        "os.makedirs(test_labels_dir, exist_ok=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8CXow-6B7YzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created the respective folders, let's start importing the images."
      ],
      "metadata": {
        "id": "MM4-17Nj7YzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of images and labels\n",
        "images = [f for f in os.listdir(images_dir) if f.endswith('.jpeg')]\n",
        "labels = [f.replace('.jpeg', '.txt') for f in images]"
      ],
      "metadata": {
        "trusted": true,
        "id": "EYFTktYi7YzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's preview the number of images and labels we have."
      ],
      "metadata": {
        "id": "sN_is21S7YzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Images Length: {len(images)}\")\n",
        "print(f\"Labels Length: {len(labels)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tjHCrZD-7YzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have an equal number of images and labels. Now, we will split the dataset, where the training set will contain `80%` of the images and the remaining `20%` will be in the test set."
      ],
      "metadata": {
        "id": "R2-pf7z47YzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "train_labels = [f.replace('.jpeg', '.txt') for f in train_images]\n",
        "test_labels = [f.replace('.jpeg', '.txt') for f in test_images]\n",
        "print(f\"Training set: {len(train_images)} images, {len(train_labels)} labels\")\n",
        "print(f\"Testing set: {len(test_images)} images, {len(test_labels)} labels\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "I0voKJCr7YzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After splitting the dataset, let's move the files to their respective folders."
      ],
      "metadata": {
        "id": "qijr_Yvg7YzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files to the respective directories\n",
        "def copy_files(file_list, src_path, dst_path):\n",
        "    for file in file_list:\n",
        "        shutil.copy(os.path.join(src_path, file), os.path.join(dst_path, file))"
      ],
      "metadata": {
        "trusted": true,
        "id": "gH-1lXrW7Yza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_files(train_images, images_dir, train_images_dir)\n",
        "copy_files(train_labels, labels_dir, train_labels_dir)\n",
        "copy_files(test_images, images_dir, test_images_dir)\n",
        "copy_files(test_labels, labels_dir, test_labels_dir)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HSym6al57Yza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "\n",
        "    xi1 = max(x1, x2)\n",
        "    yi1 = max(y1, y2)\n",
        "    xi2 = min(x1 + w1, x2 + w2)\n",
        "    yi2 = min(y1 + h1, y2 + h2)\n",
        "\n",
        "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    box1_area = w1 * h1\n",
        "    box2_area = w2 * h2\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area"
      ],
      "metadata": {
        "trusted": true,
        "id": "nXTuGDdp7Yza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    xi1 = max(box1[0], box2[0])\n",
        "    yi1 = max(box1[1], box2[1])\n",
        "    xi2 = min(box1[2], box2[2])\n",
        "    yi2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "    return inter_area / union_area if union_area != 0 else 0\n",
        "\n",
        "def convert_yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x_min = (x_center - width / 2) * img_width\n",
        "    x_max = (x_center + width / 2) * img_width\n",
        "    y_min = (y_center - height / 2) * img_height\n",
        "    y_max = (y_center + height / 2) * img_height\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "\n",
        "ious = []\n",
        "\n",
        "for annotation_file in annotations:\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        boxes = []\n",
        "        for line in lines:\n",
        "            _, x_center, y_center, width, height = map(float, line.split())\n",
        "            bbox = convert_yolo_to_bbox([x_center, y_center, width, height], img_width=average_width, img_height=average_height)\n",
        "            boxes.append(bbox)\n",
        "        # Calculate IoU for every pair of boxes in the same image\n",
        "        for i in range(len(boxes)):\n",
        "            for j in range(i + 1, len(boxes)):\n",
        "                iou = calculate_iou(boxes[i], boxes[j])\n",
        "                ious.append(iou)\n",
        "\n",
        "average_iou = np.mean(ious)\n",
        "print(\"Average IoU:\", average_iou)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pANb8e1d7Yzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Creation\n",
        "For our project on detecting ripe and unripe tomatoes, let's install YOLO and verify its functionality."
      ],
      "metadata": {
        "id": "PX2Sp4qr7Yzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aj1oyNkQ7Yzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's create a configuration file for YOLO. This file provides information about the dataset and the classes it contains."
      ],
      "metadata": {
        "id": "kxUalxaU7Yzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_yaml = \"\"\"\n",
        "train: /kaggle/working/train/images\n",
        "val: /kaggle/working/test/images\n",
        "\n",
        "nc: 2  # number of classes\n",
        "names: ['unripe', 'ripe']  # class names\n",
        "\"\"\"\n",
        "\n",
        "with open('/kaggle/working/dataset.yaml', 'w') as file:\n",
        "    file.write(dataset_yaml)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zHXmTEkI7Yzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the available commands in YOLOv8."
      ],
      "metadata": {
        "id": "b_mZLvar7Yze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo"
      ],
      "metadata": {
        "trusted": true,
        "id": "101lnT3i7Yze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training\n",
        "We will train our model using `130` epochs with a batch size of `16`, and we will set the image size to `640`. The [exploratory data analysis (EDA)](https://www.kaggle.com/code/sumn2u/eda-of-ripe-and-unripe-tomatoes-dataset/notebook) shows more information about images and their sizes."
      ],
      "metadata": {
        "id": "nEa2ZUYn7Yzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Load the trained YOLO model\n",
        "model = YOLO('yolov8n.pt')\n",
        "data_yaml_path = \"/kaggle/working/dataset.yaml\"\n",
        "\n",
        "results = model.train(data = data_yaml_path,\n",
        "            epochs=130,\n",
        "            imgsz=720,\n",
        "            device=0,\n",
        "            lr0=0.01,  # initial learning rate\n",
        "            lrf=0.001,  # final learning rate\n",
        "            save_period=10\n",
        "           )\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tYlMTq7E7Yzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation\n",
        "Let's plot the testing results and then perform the evaluation on the validation set."
      ],
      "metadata": {
        "id": "Ug0rNYsP7Yzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "trusted": true,
        "id": "FzLaQK_87Yzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpaths2=[]\n",
        "for dirname, _, filenames in os.walk('/kaggle/working/runs/detect/train'):\n",
        "    for filename in filenames:\n",
        "        if filename[-4:]=='.png' or filename[-4:]=='.jpg':\n",
        "            tpaths2+=[(os.path.join(dirname, filename))]\n",
        "tpaths2=sorted(tpaths2)\n",
        "\n",
        "print(tpaths2[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "kLQwfAtj7Yzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path in tpaths2:\n",
        "    image = Image.open(path)\n",
        "    image=np.array(image)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZoF0TeEw7Yzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on the validation set\n",
        "results = model.val()"
      ],
      "metadata": {
        "trusted": true,
        "id": "gnl-S2bh7Yzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Testing\n",
        "\n",
        "Let's test our model with test images."
      ],
      "metadata": {
        "id": "hqTdXDzz7Yzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the images\n",
        "images_path = '/kaggle/working/test/images'\n",
        "num_images = 5\n",
        "\n",
        "# Sample a subset of images\n",
        "images = random.sample([f for f in os.listdir(images_path) if f.endswith('.jpeg')], num_images)\n",
        "\n",
        "# Show the results on test images\n",
        "for image in images:\n",
        "    img_path = os.path.join(images_path, image)\n",
        "    results = model(img_path, conf=0.5, iou=0.6)\n",
        "    r = results[0]\n",
        "    im_array = r.plot()  # plot a BGR numpy array of predictions\n",
        "\n",
        "    # Save the image with predictions\n",
        "    save_path = f'/kaggle/working/{image}'\n",
        "    cv2.imwrite(save_path, im_array)\n",
        "\n",
        "    # Display the image\n",
        "    display(Image.open(save_path))"
      ],
      "metadata": {
        "trusted": true,
        "id": "KmFgQewu7Yzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Demo\n",
        "\n",
        "An application is built using the above model. You can visit it in [![Hugging Face Spaces](https://img.shields.io/badge/🤗%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/iamsuman/ripe-and-unripe-tomatoes-detection).\n",
        "\n",
        "\n",
        "[![Screenshot 2024-07-15 at 12.13.14 PM.jpg](attachment:f857fd59-bdfd-410b-9a87-f3fc74a658ac.jpg)](https://huggingface.co/spaces/iamsuman/ripe-and-unripe-tomatoes-detection)\n",
        "\n",
        "\n",
        "[![Screenshot 2024-07-15 at 12.15.22 PM.png](attachment:036d1a35-42ae-44de-a0ad-1f06d80d8911.png)](https://huggingface.co/spaces/iamsuman/ripe-and-unripe-tomatoes-detection)\n",
        "\n"
      ],
      "metadata": {
        "id": "95gmYHEO7Yzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7. Export Ultralytics YOLO Models for mobile or embedded devices.\n",
        "\n",
        "We need to export the models as `.tflite` and `.mlmodel` files to ensure they are compatible with mobile and embedded devices. To achieve this, we will use the Ultralytics YOLO CLI for model export."
      ],
      "metadata": {
        "id": "Ue5ekpB07Yzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Android\n",
        "!yolo export format=tflite model=\"/kaggle/working/runs/detect/train/weights/best.pt\" imgsz=320 int8 # For detection\n",
        "\n",
        "# !yolo export format=tflite model=model-cls imgsz=320 int8 # For classification"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "859qANeo7Yzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iOS\n",
        "!yolo export format=mlmodel model=\"/kaggle/working/runs/detect/train/weights/best.pt\" imgsz=320,192 half nms"
      ],
      "metadata": {
        "trusted": true,
        "id": "-46jB_eH7Yzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The converted models are passed to flutter app for detection purpose. Source code can be found [here](https://github.com/sumn2u/yolo-flutter-app/tree/main/example).\n",
        "\n",
        "[![tomato_ripness_detection.jpg](attachment:ca0d8490-7bd4-440c-bc7c-59dbf24d430c.jpg)](https://github.com/sumn2u/yolo-flutter-app/tree/main/example)\n"
      ],
      "metadata": {
        "id": "ebZFfKq17Yzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "In this way, we can use YOLO to detect ripe and unripe tomatoes. Thanks to annotation-lab for providing the tools for annotation and dataset."
      ],
      "metadata": {
        "id": "4DXv3LoF7Yzl"
      }
    }
  ]
}